analyzer: word
word:
  w2v_version: ntua
  randomize_not_supported: True
  min_tf: 2.

nn:
  seq_len: 70
  embedding:
    trainable: False
    noise_stddev: 0.1
    noise_type: gaussian
    dropout_keep_prob: 0.9
  rnn:
    dim: 150
  rnns: [150, 150]
  use_attention: True
  attention:
    dim: 64
  cnn:
    kernel_size: 5
    filter_num: 128
  cnns:
    - kernel_size: 5
      filter_num: 128
    - kernel_size: 3
      filter_num: 64
  dense: []
  dense_bk:
    - dim: 32
      activation: relu
      l2: 0.2
  max_out: #[6, 2, 2, 2]
  learning_rate:
    init: 0.01
    decay_steps: 1
    decay_rate: 0.9
  l2_reg_lambda: 0.2
  output:
    dim: _
  dropout_keep_prob: 0.5
  binary_classification: False

train:
  epoch: 200
  valid_rate: 0.1
  batch_size: 100
  early_stop_metric: f1
  train_sampling: False
  use_class_weights: True
  label_map:
    b: {0: 0, 1: 1, 2: 1, 3: 1}
    b2: {1: 0, 2: 1, 3: 1}
    b3: {2: 0, 3: 1}
    bp: {1: 0, 2: 1, 3: 2}
    b01: {0: 0, 1: 1}
    b02: {0: 0, 2: 1}
    b03: {0: 0, 3: 1}

